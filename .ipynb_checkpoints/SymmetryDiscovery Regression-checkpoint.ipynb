{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-28T13:46:00.648Z",
     "iopub.status.busy": "2020-10-28T13:46:00.643Z",
     "iopub.status.idle": "2020-10-28T13:46:00.653Z",
     "shell.execute_reply": "2020-10-28T13:46:00.622Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, ReLU, ELU, PReLU, Input, Concatenate, Lambda\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-28T13:46:00.665Z",
     "iopub.status.busy": "2020-10-28T13:46:00.661Z",
     "iopub.status.idle": "2020-10-28T13:46:00.670Z",
     "shell.execute_reply": "2020-10-28T13:46:00.627Z"
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def myloss(y_true, y_pred, alpha = 0.0):\n",
    "    #alpha determines the amount of decorrelation; 0 means no decorrelation.\n",
    "    \n",
    "    #We want to learn f(g(x)) = x with g != identity and g(x) and x should have the same probability density.\n",
    "    #g(x) = y_pred[:,0]\n",
    "    #f(g(x)) = y_pred[:,1]\n",
    "    #h(x) = y_pred[:,2]\n",
    "    #h(g(x)) = y_pred[:,3]\n",
    "    \n",
    "    myoutput =  mse(y_true[:,0],y_pred[:,1]) \\\n",
    "                - alpha*binary_crossentropy(y_pred[:,2],K.ones_like(y_pred[:,0])) \\\n",
    "                - alpha*binary_crossentropy(y_pred[:,3],0.*K.ones_like(y_pred[:,0]))\n",
    "    return myoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-28T13:46:00.682Z",
     "iopub.status.busy": "2020-10-28T13:46:00.676Z",
     "iopub.status.idle": "2020-10-28T13:46:00.687Z",
     "shell.execute_reply": "2020-10-28T13:46:00.634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on i= 0\n",
      "200/200 [==============================] - 0s 691us/step - loss: 0.4622\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6563\n",
      "on i= 1\n",
      "200/200 [==============================] - 0s 689us/step - loss: 0.9495\n",
      "100/100 [==============================] - 0s 842us/step - loss: 0.0805\n",
      "on i= 2\n",
      "200/200 [==============================] - 0s 842us/step - loss: 0.6909\n",
      "100/100 [==============================] - 0s 883us/step - loss: 0.0508\n",
      "on i= 3\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.6903\n",
      "100/100 [==============================] - 0s 875us/step - loss: 0.0360\n",
      "on i= 4\n",
      "200/200 [==============================] - 0s 694us/step - loss: 0.6924\n",
      "100/100 [==============================] - 0s 870us/step - loss: 0.0240\n",
      "on i= 5\n",
      "200/200 [==============================] - 0s 699us/step - loss: 0.6929\n",
      "100/100 [==============================] - 0s 864us/step - loss: 0.0140\n",
      "on i= 6\n",
      "200/200 [==============================] - 0s 699us/step - loss: 0.6942\n",
      "100/100 [==============================] - 0s 918us/step - loss: 0.0052\n",
      "on i= 7\n",
      "200/200 [==============================] - 0s 668us/step - loss: 0.6940\n",
      "100/100 [==============================] - 0s 901us/step - loss: 8.3746e-04\n",
      "on i= 8\n",
      "200/200 [==============================] - 0s 695us/step - loss: 0.6937\n",
      "100/100 [==============================] - 0s 887us/step - loss: 2.6295e-04\n",
      "on i= 9\n",
      "200/200 [==============================] - 0s 713us/step - loss: 0.6920\n",
      "100/100 [==============================] - 0s 918us/step - loss: 1.7863e-04\n",
      "on i= 0\n",
      "200/200 [==============================] - 0s 702us/step - loss: 0.5321\n",
      "100/100 [==============================] - 0s 833us/step - loss: 0.3784\n",
      "on i= 1\n",
      "200/200 [==============================] - 0s 691us/step - loss: 0.8194\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0873\n",
      "on i= 2\n",
      "200/200 [==============================] - 0s 658us/step - loss: 0.6910\n",
      "100/100 [==============================] - 0s 859us/step - loss: 0.0521\n",
      "on i= 3\n",
      "200/200 [==============================] - 0s 694us/step - loss: 0.6921\n",
      "100/100 [==============================] - 0s 834us/step - loss: 0.0335\n",
      "on i= 4\n",
      "200/200 [==============================] - 0s 706us/step - loss: 0.6926\n",
      "100/100 [==============================] - 0s 967us/step - loss: 0.0228\n",
      "on i= 5\n",
      "200/200 [==============================] - 0s 696us/step - loss: 0.6930\n",
      "100/100 [==============================] - 0s 933us/step - loss: 0.0159\n",
      "on i= 6\n",
      "200/200 [==============================] - 0s 697us/step - loss: 0.6940\n",
      "100/100 [==============================] - 0s 887us/step - loss: 0.0121\n",
      "on i= 7\n",
      "200/200 [==============================] - 0s 703us/step - loss: 0.6943\n",
      "100/100 [==============================] - 0s 918us/step - loss: 0.0088\n",
      "on i= 8\n",
      "200/200 [==============================] - 0s 698us/step - loss: 0.6942\n",
      "100/100 [==============================] - 0s 910us/step - loss: 0.0064\n",
      "on i= 9\n",
      "200/200 [==============================] - 0s 709us/step - loss: 0.6948\n",
      "100/100 [==============================] - 0s 885us/step - loss: 0.0047\n",
      "on i= 0\n",
      "200/200 [==============================] - 0s 688us/step - loss: 0.5078\n",
      "100/100 [==============================] - 0s 757us/step - loss: 0.5692\n",
      "on i= 1\n",
      "200/200 [==============================] - 0s 654us/step - loss: 0.7958\n",
      "100/100 [==============================] - 0s 782us/step - loss: 0.1258\n",
      "on i= 2\n",
      "200/200 [==============================] - 0s 633us/step - loss: 0.6737\n",
      "100/100 [==============================] - 0s 793us/step - loss: 0.0672\n",
      "on i= 3\n",
      "200/200 [==============================] - 0s 641us/step - loss: 0.6742\n",
      "100/100 [==============================] - 0s 779us/step - loss: 0.0366\n",
      "on i= 4\n",
      "200/200 [==============================] - 0s 642us/step - loss: 0.6738\n",
      "100/100 [==============================] - 0s 823us/step - loss: 0.0187\n",
      "on i= 5\n",
      "200/200 [==============================] - 0s 712us/step - loss: 0.6765\n",
      "100/100 [==============================] - 0s 860us/step - loss: 0.0097\n",
      "on i= 6\n",
      "200/200 [==============================] - 0s 674us/step - loss: 0.6782\n",
      "100/100 [==============================] - 0s 905us/step - loss: 0.0043\n",
      "on i= 7\n",
      "200/200 [==============================] - 0s 697us/step - loss: 0.6816\n",
      "100/100 [==============================] - 0s 841us/step - loss: 0.0018\n",
      "on i= 8\n",
      "200/200 [==============================] - 0s 671us/step - loss: 0.6835\n",
      "100/100 [==============================] - 0s 896us/step - loss: 9.3518e-04\n",
      "on i= 9\n",
      "200/200 [==============================] - 0s 723us/step - loss: 0.6842\n",
      "100/100 [==============================] - 0s 926us/step - loss: 5.4042e-04\n",
      "on i= 0\n",
      "200/200 [==============================] - 0s 713us/step - loss: 0.5586\n",
      "100/100 [==============================] - 0s 925us/step - loss: 0.3853\n",
      "on i= 1\n",
      "200/200 [==============================] - 0s 699us/step - loss: 0.8188\n",
      "100/100 [==============================] - 0s 938us/step - loss: 0.1218\n",
      "on i= 2\n",
      "200/200 [==============================] - 0s 701us/step - loss: 0.6930\n",
      "100/100 [==============================] - 0s 959us/step - loss: 0.0698\n",
      "on i= 3\n",
      "200/200 [==============================] - 0s 714us/step - loss: 0.6911\n",
      "100/100 [==============================] - 0s 824us/step - loss: 0.0401\n",
      "on i= 4\n",
      "200/200 [==============================] - 0s 726us/step - loss: 0.6915\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "on i= 5\n",
      "200/200 [==============================] - 0s 2ms/step - loss: 0.6916A: 0s - loss: 0\n",
      "100/100 [==============================] - 0s 801us/step - loss: 0.0171\n",
      "on i= 6\n",
      "200/200 [==============================] - 0s 790us/step - loss: 0.6929\n",
      "100/100 [==============================] - 0s 933us/step - loss: 0.0115\n",
      "on i= 7\n",
      "200/200 [==============================] - 0s 743us/step - loss: 0.6933\n",
      "100/100 [==============================] - 0s 836us/step - loss: 0.0072\n",
      "on i= 8\n",
      "200/200 [==============================] - 0s 713us/step - loss: 0.6944\n",
      "100/100 [==============================] - 0s 808us/step - loss: 0.0043\n",
      "on i= 9\n",
      "200/200 [==============================] - 0s 726us/step - loss: 0.6945\n",
      "100/100 [==============================] - 0s 761us/step - loss: 0.0027\n",
      "on i= 0\n",
      "200/200 [==============================] - 0s 641us/step - loss: 0.5628\n",
      "100/100 [==============================] - 0s 827us/step - loss: 0.2737\n",
      "on i= 1\n",
      "200/200 [==============================] - 0s 851us/step - loss: 0.8548\n",
      "100/100 [==============================] - 0s 761us/step - loss: 0.0505\n",
      "on i= 2\n",
      "200/200 [==============================] - 0s 614us/step - loss: 0.6901\n",
      "100/100 [==============================] - 0s 735us/step - loss: 0.0254\n",
      "on i= 3\n",
      "200/200 [==============================] - 0s 630us/step - loss: 0.6754\n",
      "100/100 [==============================] - 0s 726us/step - loss: 0.0075\n",
      "on i= 4\n",
      "200/200 [==============================] - 0s 624us/step - loss: 0.6604\n",
      "100/100 [==============================] - 0s 729us/step - loss: 0.0030\n",
      "on i= 5\n",
      "200/200 [==============================] - 0s 816us/step - loss: 0.6558\n",
      "100/100 [==============================] - 0s 769us/step - loss: 0.0022\n",
      "on i= 6\n",
      "200/200 [==============================] - 0s 634us/step - loss: 0.6567\n",
      "100/100 [==============================] - 0s 786us/step - loss: 0.0018\n",
      "on i= 7\n",
      "200/200 [==============================] - 0s 624us/step - loss: 0.6507\n",
      "100/100 [==============================] - 0s 761us/step - loss: 0.0013\n",
      "on i= 8\n",
      "200/200 [==============================] - 0s 651us/step - loss: 0.6346\n",
      "100/100 [==============================] - 0s 724us/step - loss: 8.4648e-04\n",
      "on i= 9\n",
      "200/200 [==============================] - 0s 641us/step - loss: 0.6104\n",
      "100/100 [==============================] - 0s 783us/step - loss: 4.9545e-04\n",
      "on i= 0\n",
      "200/200 [==============================] - 0s 682us/step - loss: 0.4393\n",
      "100/100 [==============================] - 0s 801us/step - loss: 0.4852\n",
      "on i= 1\n",
      "200/200 [==============================] - 0s 675us/step - loss: 1.3919\n",
      "100/100 [==============================] - 0s 923us/step - loss: 0.0512\n",
      "on i= 2\n",
      "200/200 [==============================] - 0s 644us/step - loss: 0.6925\n",
      "100/100 [==============================] - 0s 826us/step - loss: 0.0373\n",
      "on i= 3\n",
      "200/200 [==============================] - 0s 669us/step - loss: 0.6924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 778us/step - loss: 0.0273\n",
      "on i= 4\n",
      "200/200 [==============================] - 0s 661us/step - loss: 0.6939\n",
      "100/100 [==============================] - 0s 800us/step - loss: 0.0181\n",
      "on i= 5\n",
      "200/200 [==============================] - 0s 647us/step - loss: 0.6939\n",
      "100/100 [==============================] - 0s 793us/step - loss: 0.0096\n",
      "on i= 6\n",
      "200/200 [==============================] - 0s 643us/step - loss: 0.6950\n",
      "100/100 [==============================] - 0s 820us/step - loss: 0.0032\n",
      "on i= 7\n",
      "200/200 [==============================] - 0s 662us/step - loss: 0.6966\n",
      "100/100 [==============================] - 0s 816us/step - loss: 6.1396e-04\n",
      "on i= 8\n",
      "200/200 [==============================] - 0s 661us/step - loss: 0.6959\n",
      "100/100 [==============================] - 0s 831us/step - loss: 1.9739e-04\n",
      "on i= 9\n",
      "200/200 [==============================] - 0s 655us/step - loss: 0.6958\n",
      "100/100 [==============================] - 0s 796us/step - loss: 9.5468e-05\n",
      "on i= 0\n",
      "200/200 [==============================] - 0s 641us/step - loss: 0.5117\n",
      "100/100 [==============================] - 0s 786us/step - loss: 0.4093\n",
      "on i= 1\n",
      "200/200 [==============================] - 0s 654us/step - loss: 0.9781\n",
      "100/100 [==============================] - 0s 779us/step - loss: 0.0762\n",
      "on i= 2\n",
      "200/200 [==============================] - 0s 641us/step - loss: 0.6931\n",
      "100/100 [==============================] - 0s 734us/step - loss: 0.0486\n",
      "on i= 3\n",
      "200/200 [==============================] - 0s 657us/step - loss: 0.6940\n",
      "100/100 [==============================] - 0s 751us/step - loss: 0.0308\n",
      "on i= 4\n",
      "200/200 [==============================] - 0s 676us/step - loss: 0.6943\n",
      "100/100 [==============================] - 0s 805us/step - loss: 0.0178\n",
      "on i= 5\n",
      "200/200 [==============================] - 0s 703us/step - loss: 0.6934\n",
      "100/100 [==============================] - 0s 814us/step - loss: 0.0088\n",
      "on i= 6\n",
      "200/200 [==============================] - 0s 660us/step - loss: 0.6885\n",
      "100/100 [==============================] - 0s 846us/step - loss: 0.0042\n",
      "on i= 7\n",
      "200/200 [==============================] - 0s 670us/step - loss: 0.6850\n",
      "100/100 [==============================] - 0s 775us/step - loss: 0.0023\n",
      "on i= 8\n",
      "200/200 [==============================] - 0s 797us/step - loss: 0.6893\n",
      "100/100 [==============================] - 0s 837us/step - loss: 0.0013\n",
      "on i= 9\n",
      "200/200 [==============================] - 0s 684us/step - loss: 0.6918\n",
      "100/100 [==============================] - 0s 837us/step - loss: 7.4736e-04\n",
      "on i= 0\n",
      "200/200 [==============================] - 0s 654us/step - loss: 0.4724\n",
      "100/100 [==============================] - 0s 792us/step - loss: 0.4340\n",
      "on i= 1\n",
      "200/200 [==============================] - 0s 646us/step - loss: 1.0847\n",
      "100/100 [==============================] - 0s 901us/step - loss: 0.1027\n",
      "on i= 2\n",
      "200/200 [==============================] - 0s 641us/step - loss: 0.6923\n",
      "100/100 [==============================] - 0s 834us/step - loss: 0.0797\n",
      "on i= 3\n",
      "200/200 [==============================] - 0s 702us/step - loss: 0.6895\n",
      "100/100 [==============================] - 0s 811us/step - loss: 0.0634\n",
      "on i= 4\n",
      "200/200 [==============================] - 0s 663us/step - loss: 0.6903\n",
      "100/100 [==============================] - 0s 833us/step - loss: 0.0489\n",
      "on i= 5\n",
      "200/200 [==============================] - 0s 633us/step - loss: 0.6914\n",
      "100/100 [==============================] - 0s 832us/step - loss: 0.0354\n",
      "on i= 6\n",
      "200/200 [==============================] - 0s 674us/step - loss: 0.6911\n",
      "100/100 [==============================] - 0s 860us/step - loss: 0.0218\n",
      "on i= 7\n",
      "200/200 [==============================] - 0s 679us/step - loss: 0.6941\n",
      "100/100 [==============================] - 0s 844us/step - loss: 0.0107\n",
      "on i= 8\n",
      "200/200 [==============================] - 0s 675us/step - loss: 0.6961\n",
      "100/100 [==============================] - 0s 844us/step - loss: 0.0036\n",
      "on i= 9\n",
      "200/200 [==============================] - 0s 677us/step - loss: 0.6968\n",
      "100/100 [==============================] - 0s 814us/step - loss: 0.0015\n",
      "on i= 0\n",
      "200/200 [==============================] - 0s 662us/step - loss: 0.4403\n",
      "100/100 [==============================] - 0s 811us/step - loss: 0.6764\n",
      "on i= 1\n",
      "200/200 [==============================] - 0s 667us/step - loss: 1.0522\n",
      "100/100 [==============================] - 0s 984us/step - loss: 0.0773\n",
      "on i= 2\n",
      "200/200 [==============================] - 0s 634us/step - loss: 0.6827\n",
      "100/100 [==============================] - 0s 839us/step - loss: 0.0470\n",
      "on i= 3\n",
      "200/200 [==============================] - 0s 690us/step - loss: 0.6804\n",
      "100/100 [==============================] - 0s 841us/step - loss: 0.0317\n",
      "on i= 4\n",
      "200/200 [==============================] - 0s 700us/step - loss: 0.6832\n",
      "100/100 [==============================] - 0s 837us/step - loss: 0.0202\n",
      "on i= 5\n",
      "200/200 [==============================] - 0s 684us/step - loss: 0.6852\n",
      "100/100 [==============================] - 0s 808us/step - loss: 0.0118\n",
      "on i= 6\n",
      "200/200 [==============================] - 0s 660us/step - loss: 0.6854\n",
      "100/100 [==============================] - 0s 837us/step - loss: 0.0069\n",
      "on i= 7\n",
      "200/200 [==============================] - 0s 683us/step - loss: 0.6857\n",
      "100/100 [==============================] - 0s 831us/step - loss: 0.0039\n",
      "on i= 8\n",
      "200/200 [==============================] - 0s 701us/step - loss: 0.6851\n",
      "100/100 [==============================] - 0s 938us/step - loss: 0.0022\n",
      "on i= 9\n",
      "200/200 [==============================] - 0s 684us/step - loss: 0.6844\n",
      "100/100 [==============================] - 0s 829us/step - loss: 0.0012\n",
      "on i= 0\n",
      "200/200 [==============================] - 0s 711us/step - loss: 0.5120\n",
      "100/100 [==============================] - 0s 835us/step - loss: 0.5613\n",
      "on i= 1\n",
      "200/200 [==============================] - 0s 699us/step - loss: 0.9162\n",
      "100/100 [==============================] - 0s 827us/step - loss: 0.0850\n",
      "on i= 2\n",
      "200/200 [==============================] - 0s 708us/step - loss: 0.6851\n",
      "100/100 [==============================] - 0s 827us/step - loss: 0.0509\n",
      "on i= 3\n",
      "200/200 [==============================] - 0s 707us/step - loss: 0.6831\n",
      "100/100 [==============================] - 0s 848us/step - loss: 0.0325\n",
      "on i= 4\n",
      "200/200 [==============================] - 0s 659us/step - loss: 0.6784\n",
      "100/100 [==============================] - 0s 832us/step - loss: 0.0190\n",
      "on i= 5\n",
      "200/200 [==============================] - 0s 673us/step - loss: 0.6739\n",
      "100/100 [==============================] - 0s 828us/step - loss: 0.0093\n",
      "on i= 6\n",
      "200/200 [==============================] - 0s 676us/step - loss: 0.6708\n",
      "100/100 [==============================] - 0s 839us/step - loss: 0.0037\n",
      "on i= 7\n",
      "200/200 [==============================] - 0s 690us/step - loss: 0.6689\n",
      "100/100 [==============================] - 0s 821us/step - loss: 0.0012\n",
      "on i= 8\n",
      "200/200 [==============================] - 0s 689us/step - loss: 0.6660\n",
      "100/100 [==============================] - 0s 833us/step - loss: 5.2131e-04\n",
      "on i= 9\n",
      "200/200 [==============================] - 0s 736us/step - loss: 0.6704\n",
      "100/100 [==============================] - 0s 800us/step - loss: 2.8611e-04\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10\n",
    "identity = 0\n",
    "neg_identity = 0\n",
    "other = 0\n",
    "error = 0.05\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    \n",
    "    x = mydata = np.random.normal(0, 1, 10000)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #setup the model\n",
    "\n",
    "    myinput = Input(shape=(1,))\n",
    "    encoded = Dense(5, activation='elu')(myinput)\n",
    "    encoded = Dense(5, activation='elu')(encoded)\n",
    "    encoded = Dense(1, activation='linear')(encoded)\n",
    "\n",
    "    encoder = Model(myinput, encoded)\n",
    "    encoder2 = encoder(encoder(myinput))\n",
    "    autoencoder = Model(myinput, encoder2)\n",
    "\n",
    "    combinedModel = Model(myinput,Concatenate(axis=-1)([encoded, encoder2]))\n",
    "\n",
    "    myinput_classify = Input(shape=(1,))\n",
    "    myclassifier = Dense(128, activation='elu')(myinput_classify)\n",
    "    myclassifier = Dense(64, activation='elu')(myclassifier)\n",
    "    myclassifier = Dense(1, activation='sigmoid')(myclassifier)\n",
    "    myclassifier_model = Model(myinput_classify, myclassifier)\n",
    "    myclassifier_input = myclassifier_model(myinput)\n",
    "    myclassifier_encoded = myclassifier_model(encoded)\n",
    "\n",
    "    combinedModel_classifier = Model(myinput,Concatenate(axis=-1)([encoded, encoder2, myclassifier_input, myclassifier_encoded]))\n",
    "\n",
    "    #First, pretrain the autoencoder:\n",
    "    #autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    #autoencoder.fit(x,x, epochs=5, batch_size=int(0.01*len(x)))\n",
    "\n",
    "    preds = []\n",
    "    preds += [encoder.predict(np.linspace(-4,4,10))]\n",
    "\n",
    "    for i in range(10):\n",
    "        print(\"on i=\",i)\n",
    "        #Now, train the classifier\n",
    "        encoded_x = encoder.predict(x)[:,0]\n",
    "        myclassifier_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "        myclassifier_model.fit(np.concatenate([x,encoded_x]),np.concatenate([np.ones(len(x)),np.zeros(len(x))]), epochs=1, batch_size=int(0.01*len(x)))\n",
    "\n",
    "        #y_scores = encoder.predict(np.concatenate([x,encoded_x]))\n",
    "        #print(i,roc_auc_score(np.concatenate([np.ones(len(x)),np.zeros(len(x))]),y_scores))\n",
    "\n",
    "        #Now, update the autoencoder\n",
    "        for layer in myclassifier_model.layers[:]:\n",
    "            layer.trainable = False\n",
    "\n",
    "        combinedModel_classifier.compile(optimizer='adam', loss=lambda y_true, y_pred: myloss(y_true, y_pred))\n",
    "        combinedModel_classifier.fit(x,np.stack((x, x), axis=-1), epochs=1, batch_size=int(0.01*len(x)))\n",
    "\n",
    "        preds += [encoder.predict(np.linspace(-4,4,10))]\n",
    "\n",
    "        for layer in myclassifier_model.layers[:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "\n",
    "    #for i in range(len(preds)):\n",
    "     #   plt.plot(np.linspace(-4,4,10),preds[i],label=str(i))\n",
    "      #  pass\n",
    "    #plt.legend(frameon=False)\n",
    "\n",
    "\n",
    "    mypreds = combinedModel_classifier.predict(x)\n",
    "\n",
    "    model = LinearRegression().fit(x.reshape((-1, 1)), mypreds[:,0])\n",
    "    #r_sq = model.score(x.reshape((-1, 1)), mypreds[:,0])\n",
    "    #print('coefficient of determination:', r_sq)\n",
    "    #print('intercept:', model.intercept_)\n",
    "    #print('slope:', model.coef_)\n",
    "    \n",
    "    if abs(model.coef_ - 1) <= error:\n",
    "        identity += 1\n",
    "    elif abs(model.coef_ + 1) <= error:\n",
    "        neg_identity += 1\n",
    "    else:\n",
    "        other += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity =  4\n",
      "neg_identity =  6\n",
      "other =  0\n"
     ]
    }
   ],
   "source": [
    "print(\"identity = \", identity)\n",
    "print(\"neg_identity = \", neg_identity)\n",
    "print(\"other = \", other)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nteract": {
   "version": "0.26.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
